{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gv_fea_dict(alpha, feature, df, train_df):\n",
    "\n",
    "        value_count = train_df[feature].value_counts()\n",
    "\n",
    "        gv_dict = dict()\n",
    "        for i, denominator in value_count.items():\n",
    "            vec = []\n",
    "            for k in range(1,10):\n",
    "                cls_cnt = train_df.loc[(train_df['Class']==k) & (train_df[feature]==i)]\n",
    "                vec.append((cls_cnt.shape[0] + alpha*10)/ (denominator + 90*alpha))\n",
    "            gv_dict[i]=vec\n",
    "        return gv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gv_feature(alpha, feature, df, train_df):\n",
    "\n",
    "        gv_dict = get_gv_fea_dict(alpha, feature, df, train_df)\n",
    "        value_count = train_df[feature].value_counts()\n",
    "        gv_fea = []\n",
    "        for index, row in df.iterrows():\n",
    "            if row[feature] in dict(value_count).keys():\n",
    "                gv_fea.append(gv_dict[row[feature]])\n",
    "            else:\n",
    "                gv_fea.append([1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9,1/9])\n",
    "        return gv_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dictionary_paddle(cls_text):\n",
    "        dictionary = defaultdict(int)\n",
    "        for index, row in cls_text.iterrows():\n",
    "            for word in row['TEXT'].split():\n",
    "                dictionary[word] +=1\n",
    "        return dictionary    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_responsecoding(df, dict_list, total_dict):\n",
    "        text_feature_responseCoding = np.zeros((df.shape[0],9))\n",
    "        for i in range(0,9):\n",
    "            row_index = 0\n",
    "            for index, row in df.iterrows():\n",
    "                sum_prob = 0\n",
    "                for word in row['TEXT'].split():\n",
    "                    sum_prob += math.log(((dict_list[i].get(word,0)+10 )/(total_dict.get(word,0)+90)))\n",
    "                text_feature_responseCoding[row_index][i] = math.exp(sum_prob/len(row['TEXT'].split()))\n",
    "                row_index += 1\n",
    "        return text_feature_responseCoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersec_text(df):\n",
    "        df_text_vec = CountVectorizer(min_df=3)\n",
    "        df_text_fea = df_text_vec.fit_transform(df['TEXT'])\n",
    "        df_text_features = df_text_vec.get_feature_names()\n",
    "\n",
    "        df_text_fea_counts = df_text_fea.sum(axis=0).A1\n",
    "        df_text_fea_dict = dict(zip(list(df_text_features),df_text_fea_counts))\n",
    "        len1 = len(set(df_text_features))\n",
    "        len2 = len(set(train_text_features) & set(df_text_features))\n",
    "        return len1,len2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_something(train_x, train_y,test_x, test_y, clf):\n",
    "        clf.fit(train_x, train_y)\n",
    "        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "        sig_clf.fit(train_x, train_y)\n",
    "        pred_y = sig_clf.predict(test_x)\n",
    "\n",
    "        # for calculating log_loss we willl provide the array of probabilities belongs to each class\n",
    "        log_loss_value = log_loss(test_y, sig_clf.predict_proba(test_x))\n",
    "        # calculating the number of data points that are misclassified\n",
    "        number_of_misclassified_points = np.count_nonzero((pred_y- test_y))/test_y.shape[0]\n",
    "        \n",
    "        return log_loss_value, number_of_misclassified_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_log_loss(train_x, train_y, test_x, test_y,  clf):\n",
    "        clf.fit(train_x, train_y)\n",
    "        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "        sig_clf.fit(train_x, train_y)\n",
    "        sig_clf_probs = sig_clf.predict_proba(test_x)\n",
    "        return log_loss(test_y, sig_clf_probs, eps=1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impfeature_names(indices, text, gene, var, no_features, train_df):\n",
    "        gene_count_vec = CountVectorizer()\n",
    "        var_count_vec = CountVectorizer()\n",
    "        text_count_vec = CountVectorizer(min_df=3)\n",
    "\n",
    "        gene_vec = gene_count_vec.fit(train_df['Gene'])\n",
    "        var_vec  = var_count_vec.fit(train_df['Variation'])\n",
    "        text_vec = text_count_vec.fit(train_df['TEXT'])\n",
    "\n",
    "        fea1_len = len(gene_vec.get_feature_names())\n",
    "        fea2_len = len(var_count_vec.get_feature_names())\n",
    "\n",
    "        word_present = 0\n",
    "        for i,v in enumerate(indices):\n",
    "            if (v < fea1_len):\n",
    "                word = gene_vec.get_feature_names()[v]\n",
    "                yes_no = True if word == gene else False\n",
    "                if yes_no:\n",
    "                    word_present += 1\n",
    "    \n",
    "            elif (v < fea1_len+fea2_len):\n",
    "                word = var_vec.get_feature_names()[v-(fea1_len)]\n",
    "                yes_no = True if word == var else False\n",
    "                if yes_no:\n",
    "                    word_present += 1\n",
    "           \n",
    "            else:\n",
    "                word = text_vec.get_feature_names()[v-(fea1_len+fea2_len)]\n",
    "                yes_no = True if word in text.split() else False\n",
    "                if yes_no:\n",
    "                    word_present += 1\n",
    "\n",
    "        return no_features, word_present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Da Magic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_csv_to_something_good(dataset, type_of_output = 'Onehotencoding'):\n",
    "    result = pd.read_csv(dataset)\n",
    "    y_true = result['Class'].values\n",
    "    X_train, test_df, y_train, y_test = train_test_split(result, y_true, stratify=y_true, test_size=0.2)\n",
    "    # split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\n",
    "    train_df, cv_df, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)\n",
    "\n",
    "    # get_gv_fea_dict: Get Gene varaition Feature Dict\n",
    "    alpha = 1\n",
    "    # train gene feature\n",
    "    train_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", train_df, train_df))\n",
    "    # test gene feature\n",
    "    test_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", test_df, train_df))\n",
    "    # cross validation gene feature\n",
    "    cv_gene_feature_responseCoding = np.array(get_gv_feature(alpha, \"Gene\", cv_df, train_df))\n",
    "\n",
    "    # one-hot encoding of Gene feature.\n",
    "    gene_vectorizer = CountVectorizer()\n",
    "    train_gene_feature_onehotCoding = gene_vectorizer.fit_transform(train_df['Gene'])\n",
    "    test_gene_feature_onehotCoding = gene_vectorizer.transform(test_df['Gene'])\n",
    "    cv_gene_feature_onehotCoding = gene_vectorizer.transform(cv_df['Gene'])\n",
    "\n",
    "    # alpha is used for laplace smoothing\n",
    "    alpha = 1\n",
    "    # train gene feature\n",
    "    train_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", train_df, train_df))\n",
    "    # test gene feature\n",
    "    test_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", test_df, train_df))\n",
    "    # cross validation gene feature\n",
    "    cv_variation_feature_responseCoding = np.array(get_gv_feature(alpha, \"Variation\", cv_df, train_df))\n",
    "\n",
    "    # one-hot encoding of variation feature.\n",
    "    variation_vectorizer = CountVectorizer()\n",
    "    train_variation_feature_onehotCoding = variation_vectorizer.fit_transform(train_df['Variation'])\n",
    "    test_variation_feature_onehotCoding = variation_vectorizer.transform(test_df['Variation'])\n",
    "    cv_variation_feature_onehotCoding = variation_vectorizer.transform(cv_df['Variation'])\n",
    "\n",
    "\n",
    "    text_vectorizer = CountVectorizer(min_df=3)\n",
    "    train_text_feature_onehotCoding = text_vectorizer.fit_transform(train_df['TEXT'])\n",
    "    # getting all the feature names (words)\n",
    "    train_text_features= text_vectorizer.get_feature_names()\n",
    "\n",
    "    # train_text_feature_onehotCoding.sum(axis=0).A1 will sum every row and returns (1*number of features) vector\n",
    "    train_text_fea_counts = train_text_feature_onehotCoding.sum(axis=0).A1\n",
    "\n",
    "    # zip(list(text_features),text_fea_counts) will zip a word with its number of times it occured\n",
    "    text_fea_dict = dict(zip(list(train_text_features),train_text_fea_counts))\n",
    "\n",
    "\n",
    "    dict_list = []\n",
    "    # dict_list =[] contains 9 dictoinaries each corresponds to a class\n",
    "    for i in range(1,10):\n",
    "        cls_text = train_df[train_df['Class']==i]\n",
    "        # build a word dict based on the words in that class\n",
    "        dict_list.append(extract_dictionary_paddle(cls_text))\n",
    "\n",
    "    total_dict = extract_dictionary_paddle(train_df)\n",
    "\n",
    "    #response coding of text features\n",
    "    train_text_feature_responseCoding  = get_text_responsecoding(train_df, dict_list, total_dict)\n",
    "    test_text_feature_responseCoding  = get_text_responsecoding(test_df, dict_list, total_dict)\n",
    "    cv_text_feature_responseCoding  = get_text_responsecoding(cv_df, dict_list, total_dict)\n",
    "\n",
    "    # https://stackoverflow.com/a/16202486\n",
    "    # we convert each row values such that they sum to 1  \n",
    "    train_text_feature_responseCoding = (train_text_feature_responseCoding.T/train_text_feature_responseCoding.sum(axis=1)).T\n",
    "    test_text_feature_responseCoding = (test_text_feature_responseCoding.T/test_text_feature_responseCoding.sum(axis=1)).T\n",
    "    cv_text_feature_responseCoding = (cv_text_feature_responseCoding.T/cv_text_feature_responseCoding.sum(axis=1)).T\n",
    "\n",
    "    # don't forget to normalize every feature\n",
    "    train_text_feature_onehotCoding = normalize(train_text_feature_onehotCoding, axis=0)\n",
    "\n",
    "    # we use the same vectorizer that was trained on train data\n",
    "    test_text_feature_onehotCoding = text_vectorizer.transform(test_df['TEXT'])\n",
    "    # don't forget to normalize every feature\n",
    "    test_text_feature_onehotCoding = normalize(test_text_feature_onehotCoding, axis=0)\n",
    "\n",
    "    # we use the same vectorizer that was trained on train data\n",
    "    cv_text_feature_onehotCoding = text_vectorizer.transform(cv_df['TEXT'])\n",
    "    # don't forget to normalize every feature\n",
    "    cv_text_feature_onehotCoding = normalize(cv_text_feature_onehotCoding, axis=0)\n",
    "\n",
    "    #https://stackoverflow.com/a/2258273/4084039\n",
    "    sorted_text_fea_dict = dict(sorted(text_fea_dict.items(), key=lambda x: x[1] , reverse=True))\n",
    "    sorted_text_occur = np.array(list(sorted_text_fea_dict.values()))\n",
    "\n",
    "    train_gene_var_onehotCoding = hstack((train_gene_feature_onehotCoding,train_variation_feature_onehotCoding))\n",
    "    test_gene_var_onehotCoding = hstack((test_gene_feature_onehotCoding,test_variation_feature_onehotCoding))\n",
    "    cv_gene_var_onehotCoding = hstack((cv_gene_feature_onehotCoding,cv_variation_feature_onehotCoding))\n",
    "\n",
    "    train_x_onehotCoding = hstack((train_gene_var_onehotCoding, train_text_feature_onehotCoding)).tocsr()\n",
    "    train_y = np.array(list(train_df['Class']))\n",
    "\n",
    "    test_x_onehotCoding = hstack((test_gene_var_onehotCoding, test_text_feature_onehotCoding)).tocsr()\n",
    "    test_y = np.array(list(test_df['Class']))\n",
    "\n",
    "    cv_x_onehotCoding = hstack((cv_gene_var_onehotCoding, cv_text_feature_onehotCoding)).tocsr()\n",
    "    cv_y = np.array(list(cv_df['Class']))\n",
    "\n",
    "\n",
    "    train_gene_var_responseCoding = np.hstack((train_gene_feature_responseCoding,train_variation_feature_responseCoding))\n",
    "    test_gene_var_responseCoding = np.hstack((test_gene_feature_responseCoding,test_variation_feature_responseCoding))\n",
    "    cv_gene_var_responseCoding = np.hstack((cv_gene_feature_responseCoding,cv_variation_feature_responseCoding))\n",
    "\n",
    "    train_x_responseCoding = np.hstack((train_gene_var_responseCoding, train_text_feature_responseCoding))\n",
    "    test_x_responseCoding = np.hstack((test_gene_var_responseCoding, test_text_feature_responseCoding))\n",
    "    cv_x_responseCoding = np.hstack((cv_gene_var_responseCoding, cv_text_feature_responseCoding))\n",
    "    \n",
    "    if type_of_output == 'Onehotencoding':\n",
    "        return train_x_onehotCoding, train_y, test_x_onehotCoding, test_y, cv_x_onehotCoding, cv_y, train_df, test_df\n",
    "    \n",
    "    if type_of_output == 'responseCoding':\n",
    "        return train_x_responseCoding, train_y, test_x_responseCoding, test_y, cv_x_responseCoding, cv_y, train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_onehotCoding, train_y, test_x_onehotCoding, test_y, cv_x_onehotCoding, cv_y, train_df, test_df = convert_csv_to_something_good('results.csv', type_of_output = 'Onehotencoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_responseCoding, train_y, test_x_responseCoding, test_y, cv_x_responseCoding, cv_y, train_df, test_df = convert_csv_to_something_good('results.csv', type_of_output = 'responseCoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayes(train_x_onehotCoding, train_y, testing_dataset_x, testing_dataset_y, test_point_index,):\n",
    "    clf = MultinomialNB(alpha=0.1)\n",
    "    clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "    \n",
    "    predicted_cls = sig_clf.predict(testing_dataset_x[test_point_index])\n",
    "\n",
    "    predicted_class_probabilities = np.round(sig_clf.predict_proba(testing_dataset_x[test_point_index]),4)\n",
    "    actual_class = testing_dataset_y[test_point_index]\n",
    "    \n",
    "    return  predicted_cls, predicted_class_probabilities, actual_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbour Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNearestNeighbour(train_x_responseCoding, train_y, testing_dataset_x, testing_dataset_y, test_point_index):\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(train_x_responseCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_responseCoding, train_y)\n",
    "\n",
    "    predicted_cls = sig_clf.predict(testing_dataset_x[0].reshape(1,-1))\n",
    "\n",
    "    predicted_class = predicted_cls[0]\n",
    "    actual_class = testing_dataset_y[test_point_index]\n",
    "\n",
    "    neighbors = clf.kneighbors(testing_dataset_x[test_point_index].reshape(1, -1), 5)\n",
    "    nearest_neighbours = train_y[neighbors[1][0]]\n",
    "\n",
    "    return predicted_class, actual_class, nearest_neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, actual_class, nearest_neighbours, frequency_of_nearest_points = KNearestNeighbour(train_x_responseCoding, train_y, test_x_responseCoding, test_y, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression_balanced(train_x_onehotCoding, train_y, testing_dataset_x, testing_dataset_y, test_point_index):\n",
    "    clf = SGDClassifier(class_weight='balanced', alpha=0.001 , penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_x_onehotCoding,train_y)\n",
    "\n",
    "    predicted_cls = clf.predict(testing_dataset_x[test_point_index])\n",
    "    predicted_class = predicted_cls[0]\n",
    "    predicted_class_probabilities = np.round(clf.predict_proba(testing_dataset_x[test_point_index]),4)\n",
    "    actual_class = testing_dataset_y[test_point_index]\n",
    "    return predicted_class, predicted_class_probabilities, actual_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression_unbalanced(train_x_onehotCoding, train_y, testing_dataset_x, testing_dataset_y, test_point_index):\n",
    "    clf = SGDClassifier(alpha=0.001 , penalty='l2', loss='log', random_state=42)\n",
    "    clf.fit(train_x_onehotCoding,train_y)\n",
    "\n",
    "    predicted_cls = clf.predict(testing_dataset_x[test_point_index])\n",
    "    predicted_class = predicted_cls[0]\n",
    "    predicted_class_probabilities = np.round(clf.predict_proba(testing_dataset_x[test_point_index]),4)\n",
    "    actual_class = testing_dataset_y[test_point_index]\n",
    "    return predicted_class, predicted_class_probabilities, actual_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearSupportVectorMachines(train_x_onehotCoding, train_y, testing_dataset_x, testing_dataset_y, test_point_index):\n",
    "    clf = SGDClassifier(alpha=0.001 , penalty='l2', loss='hinge', random_state=42)\n",
    "    clf.fit(train_x_onehotCoding,train_y)\n",
    "\n",
    "    predicted_cls = clf.predict(testing_dataset_x[test_point_index])\n",
    "    predicted_class = predicted_cls[0]\n",
    "    actual_class = testing_dataset_y[test_point_index]\n",
    "    return predicted_class, actual_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LogisticRegression_unbalanced() takes 5 positional arguments but 8 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-d75191b79d72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredicted_class\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mactual_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_present\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression_unbalanced\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_onehotCoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x_onehotCoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: LogisticRegression_unbalanced() takes 5 positional arguments but 8 were given"
     ]
    }
   ],
   "source": [
    "predicted_class,  actual_class, no_features, word_present = LogisticRegression_unbalanced(train_x_onehotCoding, train_y, test_x_onehotCoding, test_y, test_df, train_df, 2, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_Classifier_OneHotEncoding(train_x_onehotCoding, train_y, testing_dataset_x, testing_dataset_y, test_point_index):\n",
    "    clf = RandomForestClassifier(n_estimators=500 , criterion='gini', max_depth=10, random_state=42, n_jobs=-1)\n",
    "    clf.fit(train_x_onehotCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_onehotCoding, train_y)\n",
    "\n",
    "    predicted_cls = sig_clf.predict(testing_dataset_x[test_point_index])\n",
    "    predicted_cls = clf.predict(testing_dataset_x[test_point_index])\n",
    "    predicted_class = predicted_cls[0]\n",
    "    actual_class = testing_dataset_y[test_point_index]\n",
    "    return predicted_class, actual_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_class_probabilities, actual_class, no_features, word_present = Random_Forest_Classifier_OneHotEncoding(train_x_onehotCoding, train_y, test_x_onehotCoding, test_y, test_df, train_df, 2, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_Classifier_responseCoding(train_x_onehotCoding, train_y, testing_dataset_x, testing_dataset_y, test_point_index):\n",
    "    clf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=5, random_state=42, n_jobs=-1)\n",
    "    clf.fit(train_x_responseCoding, train_y)\n",
    "    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "    sig_clf.fit(train_x_responseCoding, train_y)\n",
    "\n",
    "    predicted_cls = sig_clf.predict(test_x_responseCoding[test_point_index].reshape(1,-1))\n",
    "    predicted_class = predicted_cls[0]\n",
    "    actual_class = test_y[test_point_index]\n",
    "\n",
    "    return predicted_class, actual_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class, predicted_class_probabilities, actual_class = Random_Forest_Classifier_responseCoding(train_x_responseCoding, train_y, test_x_responseCoding, test_y, test_point_index, no_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "probability estimates are not available for loss='hinge'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-5ad772d4a7c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredicted_cls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_class_probabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearSupportVectorMachines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_onehotCoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x_onehotCoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-103-c56261935905>\u001b[0m in \u001b[0;36mLinearSupportVectorMachines\u001b[1;34m(train_x_onehotCoding, train_y, testing_dataset_x, testing_dataset_y, test_point_index)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpredicted_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_dataset_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_point_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpredicted_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted_cls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mpredicted_class_probabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_dataset_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_point_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mactual_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtesting_dataset_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_point_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpredicted_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_class_probabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactual_class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\almash\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[0mhttp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mjmlr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsail\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medu\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpapers\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mvolume2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mzhang02c\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mzhang02c\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m         \"\"\"\n\u001b[1;32m-> 1027\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1028\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda3\\envs\\almash\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36m_check_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    985\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    986\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"log\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"modified_huber\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 987\u001b[1;33m             raise AttributeError(\"probability estimates are not available for\"\n\u001b[0m\u001b[0;32m    988\u001b[0m                                  \" loss=%r\" % self.loss)\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: probability estimates are not available for loss='hinge'"
     ]
    }
   ],
   "source": [
    "predicted_cls, predicted_class_probabilities, actual_class = LinearSupportVectorMachines(train_x_onehotCoding, train_y, train_x_onehotCoding, train_y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(Dataset,Test_Index,No_of_features,Algorithms):\n",
    "\n",
    "    #train_x_encoded, train_y, test_x_encoded, test_y, cv_x_encoded, cv_y = convert_csv_to_something_good('results.csv', type_of_output = 'Onehotencoding')\n",
    "#     train_x_onehotCoding, train_y, test_x_onehotCoding, test_y, cv_x_onehotCoding, cv_y, train_df, test_df=convert_csv_to_something_good(Dataset,)\n",
    "\n",
    "    if Dataset == 'Dataset1':\n",
    "        dataset_onehotCoding = [train_x_onehotCoding, train_y]\n",
    "        dataset_responseCoding = [train_x_responseCoding, train_y]\n",
    "    elif Dataset == 'Dataset2':\n",
    "        dataset_onehotCoding = [test_x_onehotCoding, test_y]\n",
    "        dataset_responseCoding = [test_x_responseCoding, test_y]\n",
    "    elif Dataset == 'Dataset3':\n",
    "        dataset_onehotCoding = [cv_x_onehotCoding, cv_y]\n",
    "        dataset_responseCoding = [cv_x_responseCoding, cv_y]\n",
    "    \n",
    "    if Algorithms == \"Naive Bayes\":\n",
    "        predicted_cls, predicted_class_probabilities, actual_class = NaiveBayes(train_x_onehotCoding, train_y, dataset_onehotCoding[0], dataset_onehotCoding[1], Test_Index)\n",
    "    elif Algorithms == \"K Nearest Neighbour Classification\":\n",
    "        predicted_cls, predicted_class_probabilities, actual_class = KNearestNeighbour(train_x_responseCoding, train_y, dataset_responseCoding[0], dataset_responseCoding[1], Test_Index)\n",
    "    elif Algorithms == \"Logistic Regression Balanced\":\n",
    "        predicted_cls, predicted_class_probabilities, actual_class = LogisticRegression_balanced(train_x_onehotCoding, train_y, dataset_onehotCoding[0], dataset_onehotCoding[1], Test_Index)\n",
    "    elif Algorithms == \"Logistic Regression Unbalanced\":\n",
    "        predicted_cls, predicted_class_probabilities, actual_class = LogisticRegression_unbalanced(train_x_onehotCoding, train_y, dataset_onehotCoding[0], dataset_onehotCoding[1], Test_Index)\n",
    "    elif Algorithms == \"Linear Support Vector Machines\":\n",
    "        predicted_cls, actual_class = LinearSupportVectorMachines(train_x_onehotCoding, train_y, dataset_onehotCoding[0], dataset_onehotCoding[1], Test_Index)\n",
    "    elif Algorithms == \"Random Forest Classifier OneHotEncoded\":\n",
    "        predicted_cls, actual_class = Random_Forest_Classifier_OneHotEncoding(train_x_onehotCoding, train_y, dataset_onehotCoding[0], dataset_onehotCoding[1], Test_Index)\n",
    "    elif Algorithms == \"Random Forest Classifier ResponseCoded\":\n",
    "        predicted_cls, actual_class = Random_Forest_Classifier_responseCoding(train_x_onehotCoding, train_y, dataset_responseCoding[0], dataset_responseCoding[1], Test_Index)\n",
    "    #prediction = mobile_net.predict(arr).flatten()\n",
    "    \n",
    "    #return (classpred,actual class,{labels[i]: float(prediction[i]) for i in range(1000)})\n",
    "    return predicted_cls, actual_class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface = gr.Interface(\n",
    "    output, \n",
    "    \n",
    "     [\n",
    "       \n",
    "        gr.inputs.Dropdown( choices=['Dataset1','Dataset2','Dataset3'], type=\"value\", label='Dataset'),\n",
    "        gr.inputs.Slider( minimum=0, maximum=100, step=1, default=0, label='Test_Index'),\n",
    "        gr.inputs.Slider( minimum=0, maximum=100, step=1, default=0, label='No_of_features'),\n",
    "        gr.inputs.Radio([\"Naive Bayes\", \"K Nearest Neighbour Classification\", \"Logistic Regression Balanced\",\"Logistic Regression Unbalanced\",\"Linear Support Vector Machines\",\"Random Forest Classifier OneHotEncoded\",\"Random Forest Classifier ResponseCoded\"],label=\"Algorithms\"),\n",
    "        \n",
    "        ],\n",
    "     [gr.outputs.Textbox(label=\"Classifed Class\"),\n",
    "     gr.outputs.Textbox(label=\"Actual Class\")],\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally at: http://127.0.0.1:7889/\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "Interface loading below...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7889/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x2230d6d99a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<Flask 'gradio.networking'>, 'http://127.0.0.1:7889/', None)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
